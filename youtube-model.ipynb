{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing ELECTRA, ALBERT, DistilBERT, and TinyBERT for YouTube Comment Sentiment Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 1. DistilBERT: Distillation-Based Compression\n",
    "\n",
    "**How:**  \n",
    "DistilBERT uses *knowledge distillation* — training a smaller student model to mimic a larger BERT teacher.\n",
    "\n",
    "**Why Interesting:**  \n",
    "- Achieves about 40% fewer parameters and 60% faster inference while retaining around 97% of BERT’s performance.  \n",
    "- Illustrates simple but effective model compression via distillation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ALBERT: Parameter Sharing & Factorization\n",
    "\n",
    "**How:**  \n",
    "ALBERT reduces model size by *sharing parameters across layers* and *factorizing embeddings*.\n",
    "\n",
    "**Why Interesting:**  \n",
    "- Dramatically reduces the number of parameters without heavily impacting capacity.  \n",
    "- Introduces *sentence-order prediction* to improve pretraining efficiency.  \n",
    "- Shows that *architectural changes* (not just distillation) can yield compact, fast, yet strong models.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ELECTRA: Efficient Pretraining via Discriminators\n",
    "\n",
    "**How:**  \n",
    "ELECTRA replaces masked token prediction with a *replaced token detection* task, where a generator replaces some tokens and a discriminator predicts which tokens were replaced.\n",
    "\n",
    "**Why Interesting:**  \n",
    "- Makes pretraining more sample-efficient and faster to converge.  \n",
    "- Smaller ELECTRA models often outperform comparable-sized BERTs despite using fewer compute resources during training.  \n",
    "- Highlights innovation in *pretraining objectives* rather than model size or architecture alone.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. TinyBERT: Distillation with Layer-Wise Compression\n",
    "\n",
    "**How:**  \n",
    "TinyBERT applies *knowledge distillation* focusing on both *transformer layer compression* and *embedding compression*, using a two-stage distillation from both the encoder and prediction layers.\n",
    "\n",
    "**Why Interesting:**  \n",
    "- Produces a very compact model with fewer layers (typically 4 or 6), substantially reducing size and latency.  \n",
    "- Maintains strong performance close to larger BERT models on various NLP tasks including sentiment analysis.  \n",
    "- Designed specifically for deployment on resource-constrained devices, balancing speed and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Compare These?\n",
    "\n",
    "- They represent **complementary approaches** to making transformers faster and lighter:  \n",
    "  - Distillation (DistilBERT, TinyBERT)  \n",
    "  - Parameter efficiency (ALBERT)  \n",
    "  - Pretraining objective redesign (ELECTRA)  \n",
    "- Comparing accuracy, speed, size, and resource consumption on the same tasks reveals trade-offs important for real applications — especially on resource-restricted devices.  \n",
    "- Helps practitioners **choose the best fit** for their particular constraints (e.g., mobile deployment vs. cloud inference).  \n",
    "- Informs **future model design** by highlighting which efficiency techniques work best in which contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:17.638165Z",
     "iopub.status.busy": "2025-05-07T01:29:17.637902Z",
     "iopub.status.idle": "2025-05-07T01:29:45.195872Z",
     "shell.execute_reply": "2025-05-07T01:29:45.195309Z",
     "shell.execute_reply.started": "2025-05-07T01:29:17.638119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification as SeqModClf\n",
    "import torch\n",
    "import numpy \n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.197444Z",
     "iopub.status.busy": "2025-05-07T01:29:45.196913Z",
     "iopub.status.idle": "2025-05-07T01:29:45.201233Z",
     "shell.execute_reply": "2025-05-07T01:29:45.200528Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.197425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.202020Z",
     "iopub.status.busy": "2025-05-07T01:29:45.201802Z",
     "iopub.status.idle": "2025-05-07T01:29:45.339219Z",
     "shell.execute_reply": "2025-05-07T01:29:45.338575Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.201996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"YoutubeCommentsDataSet.csv\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.341230Z",
     "iopub.status.busy": "2025-05-07T01:29:45.340647Z",
     "iopub.status.idle": "2025-05-07T01:29:45.362026Z",
     "shell.execute_reply": "2025-05-07T01:29:45.361367Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.341210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 18,408 rows and 2 columns: \"Comment\" (with 18,364 non-null text entries) and \"Sentiment\" (fully populated with sentiment labels). Both columns contain text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.362895Z",
     "iopub.status.busy": "2025-05-07T01:29:45.362710Z",
     "iopub.status.idle": "2025-05-07T01:29:45.375316Z",
     "shell.execute_reply": "2025-05-07T01:29:45.374654Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.362880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.376345Z",
     "iopub.status.busy": "2025-05-07T01:29:45.376071Z",
     "iopub.status.idle": "2025-05-07T01:29:45.394199Z",
     "shell.execute_reply": "2025-05-07T01:29:45.393596Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.376324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep only the comments in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(str(text)) == \"en\"\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "dataset['is_english'] = dataset['Comment'].apply(is_english)\n",
    "dataset = dataset[dataset['is_english']]\n",
    "dataset = dataset[['Comment', 'Sentiment']]\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downscale the positive value and upscale the negative value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive = dataset[dataset['Sentiment'] == 'positive']\n",
    "df_neutral = dataset[dataset['Sentiment'] == 'neutral']\n",
    "df_negative = dataset[dataset['Sentiment'] == 'negative']\n",
    "\n",
    "\n",
    "df_positive_downsampled = resample(df_positive,\n",
    "                                   replace=False,\n",
    "                                   n_samples=3319,\n",
    "                                   random_state=42)\n",
    "\n",
    "df_negative_upsampled = resample(df_negative,\n",
    "                                 replace=True,     \n",
    "                                 n_samples=3319,\n",
    "                                 random_state=42)\n",
    "\n",
    "dataset = pd.concat([df_positive_downsampled, df_neutral, df_negative_upsampled])\n",
    "dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(dataset['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "Negative = 0\n",
    "Neutral = 1 \n",
    "Positive = 2\n",
    "\n",
    "To ensure consistent and clear mapping of sentiment categories to numbers across all models, which helps fairly compare their performance since they all work with the same standardized numeric labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.395007Z",
     "iopub.status.busy": "2025-05-07T01:29:45.394778Z",
     "iopub.status.idle": "2025-05-07T01:29:45.414440Z",
     "shell.execute_reply": "2025-05-07T01:29:45.413875Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.394987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#encode train & test\n",
    "def encode_labels(dataset):\n",
    "     dataset['Sentiment'] = dataset['Sentiment'].replace({'negative':0,'neutral':1,'positive':2})\n",
    "     return dataset\n",
    "\n",
    "encoded_dataset = encode_labels(dataset)\n",
    "\n",
    "encoded_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.415227Z",
     "iopub.status.busy": "2025-05-07T01:29:45.415014Z",
     "iopub.status.idle": "2025-05-07T01:29:45.419395Z",
     "shell.execute_reply": "2025-05-07T01:29:45.418629Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.415211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_dataset = encoded_dataset.rename(columns={'Comment': 'text', 'Sentiment': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.420278Z",
     "iopub.status.busy": "2025-05-07T01:29:45.420036Z",
     "iopub.status.idle": "2025-05-07T01:29:45.433861Z",
     "shell.execute_reply": "2025-05-07T01:29:45.433183Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.420258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "Allocate 30% of  data for testing and 70% for training, which provides a balanced split that allows enough data for the model to learn while reserving a sizable portion to reliably evaluate its performance on unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.436412Z",
     "iopub.status.busy": "2025-05-07T01:29:45.436221Z",
     "iopub.status.idle": "2025-05-07T01:29:45.567180Z",
     "shell.execute_reply": "2025-05-07T01:29:45.566656Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.436398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(encoded_dataset, test_size = 0.3, \n",
    "                               random_state = 42, \n",
    "                               stratify = encoded_dataset['label'])\n",
    "\n",
    "train.to_csv(\"train.csv\", index=True)\n",
    "test.to_csv(\"test.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T00:45:07.943337Z",
     "iopub.status.busy": "2025-05-07T00:45:07.943033Z",
     "iopub.status.idle": "2025-05-07T00:45:07.947336Z",
     "shell.execute_reply": "2025-05-07T00:45:07.946383Z",
     "shell.execute_reply.started": "2025-05-07T00:45:07.943318Z"
    }
   },
   "source": [
    "## Conver pandas dataset to HuggingFace dataset\n",
    "Hugging Face offers specialized, integrated support for transformer models, including standardized evaluation metrics, easy access to pretrained models, and streamlined training/evaluation pipelines—making model comparison more efficient, consistent, and tailored for NLP tasks than general-purpose pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.567975Z",
     "iopub.status.busy": "2025-05-07T01:29:45.567807Z",
     "iopub.status.idle": "2025-05-07T01:29:45.616261Z",
     "shell.execute_reply": "2025-05-07T01:29:45.615556Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.567961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_hf = Dataset.from_pandas(train.reset_index(drop= True))\n",
    "test_hf = Dataset.from_pandas(test.reset_index(drop= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.617542Z",
     "iopub.status.busy": "2025-05-07T01:29:45.617310Z",
     "iopub.status.idle": "2025-05-07T01:29:45.625412Z",
     "shell.execute_reply": "2025-05-07T01:29:45.624700Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.617519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_hf[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.626372Z",
     "iopub.status.busy": "2025-05-07T01:29:45.626182Z",
     "iopub.status.idle": "2025-05-07T01:29:45.633410Z",
     "shell.execute_reply": "2025-05-07T01:29:45.632669Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.626358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test_hf[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.634534Z",
     "iopub.status.busy": "2025-05-07T01:29:45.634186Z",
     "iopub.status.idle": "2025-05-07T01:29:45.645350Z",
     "shell.execute_reply": "2025-05-07T01:29:45.644594Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.634513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets = DatasetDict({ 'train': train_hf, 'test' : test_hf})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T03:13:41.422212Z",
     "iopub.status.busy": "2025-05-07T03:13:41.421842Z",
     "iopub.status.idle": "2025-05-07T03:13:50.325626Z",
     "shell.execute_reply": "2025-05-07T03:13:50.324660Z",
     "shell.execute_reply.started": "2025-05-07T03:13:41.422161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer_distilbert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "model = SeqModClf.from_pretrained('distilbert-base-uncased', num_labels = 3)\n",
    "\n",
    "device  =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f'model moved to {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenizer_function(dataset):\n",
    "    comments = [str(comment) for comment in dataset['text']]\n",
    "    return tokenizer(comments, padding = 'max_length',truncation = True)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenizer_function, batched= True)\n",
    "\n",
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:53.044840Z",
     "iopub.status.busy": "2025-05-07T01:29:53.044450Z",
     "iopub.status.idle": "2025-05-07T01:49:51.750000Z",
     "shell.execute_reply": "2025-05-07T01:49:51.749387Z",
     "shell.execute_reply.started": "2025-05-07T01:29:53.044813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './distilbert',\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    eval_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'eval_f1_score',\n",
    "    greater_is_better = True,\n",
    "    report_to = 'none',\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = numpy.argmax(logits, axis = -1)\n",
    "    Accuracy_score = accuracy_score(labels,predictions)\n",
    "    F1_score = f1_score(labels, predictions, average = 'weighted')\n",
    "    return {'accuracy_score':Accuracy_score,'f1_score':F1_score}\n",
    "\n",
    "trainer_distilbert = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['test'],\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer_distilbert.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELECTRA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:49:51.750918Z",
     "iopub.status.busy": "2025-05-07T01:49:51.750673Z",
     "iopub.status.idle": "2025-05-07T01:49:54.564159Z",
     "shell.execute_reply": "2025-05-07T01:49:54.563547Z",
     "shell.execute_reply.started": "2025-05-07T01:49:51.750898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "tokenizer_electra = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "model_electra = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=3)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_electra.to(device)\n",
    "print(f'Model moved to {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def electra_tokenizer_function(dataset):\n",
    "    comments = [str(comment) for comment in dataset['text']]\n",
    "    return tokenizer_electra(comments, padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenize raw datasets for ELECTRA\n",
    "tokenized_datasets_electra = datasets.map(electra_tokenizer_function, batched=True)\n",
    "\n",
    "# Remove the original 'text' column\n",
    "tokenized_datasets_electra = tokenized_datasets_electra.remove_columns(['text'])\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_datasets_electra.set_format('torch')\n",
    "\n",
    "# Check the first tokenized example in the train split\n",
    "print(tokenized_datasets_electra['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:49:54.565164Z",
     "iopub.status.busy": "2025-05-07T01:49:54.564898Z",
     "iopub.status.idle": "2025-05-07T01:49:54.780666Z",
     "shell.execute_reply": "2025-05-07T01:49:54.779637Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.565125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./electra',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_score',\n",
    "    greater_is_better=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = numpy.argmax(logits, axis=-1)\n",
    "    Accuracy_score = accuracy_score(labels, predictions)\n",
    "    F1_score = f1_score(labels, predictions, average='weighted')\n",
    "    return {'accuracy_score': Accuracy_score, 'f1_score': F1_score}\n",
    "\n",
    "trainer_electra = Trainer(\n",
    "    model=model_electra,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_electra['train'],\n",
    "    eval_dataset=tokenized_datasets_electra['test'],\n",
    "    tokenizer=tokenizer_electra,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_electra.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.781075Z",
     "iopub.status.idle": "2025-05-07T01:49:54.781318Z",
     "shell.execute_reply": "2025-05-07T01:49:54.781220Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.781210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "\n",
    "tokenizer_albert = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "model_albert = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=3)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_albert.to(device)\n",
    "print(f'Model moved to {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load ALBERT tokenizer\n",
    "tokenizer_albert = AutoTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "def albert_tokenizer_function(dataset):\n",
    "    comments = [str(comment) for comment in dataset['text']]\n",
    "    return tokenizer_albert(comments, padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenize raw datasets for ALBERT\n",
    "tokenized_datasets_albert = datasets.map(albert_tokenizer_function, batched=True)\n",
    "\n",
    "# Remove the original 'text' column\n",
    "tokenized_datasets_albert = tokenized_datasets_albert.remove_columns(['text'])\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_datasets_albert.set_format('torch')\n",
    "\n",
    "# Check the first tokenized example in the train split\n",
    "print(tokenized_datasets_albert['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.782291Z",
     "iopub.status.idle": "2025-05-07T01:49:54.782605Z",
     "shell.execute_reply": "2025-05-07T01:49:54.782456Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.782443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./albert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_score',\n",
    "    greater_is_better=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = numpy.argmax(logits, axis=-1)\n",
    "    Accuracy_score = accuracy_score(labels, predictions)\n",
    "    F1_score = f1_score(labels, predictions, average='weighted')\n",
    "    return {'accuracy_score': Accuracy_score, 'f1_score': F1_score}\n",
    "\n",
    "trainer_albert = Trainer(\n",
    "    model=model_albert,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_albert['train'],\n",
    "    eval_dataset=tokenized_datasets_albert['test'],\n",
    "    tokenizer=tokenizer_albert,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_albert.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "okenizer_tinybert = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "model_tinybert = AutoModelForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=3)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_tinybert.to(device)\n",
    "print(f'Model moved to {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load TinyBERT tokenizer - use the appropriate TinyBERT checkpoint\n",
    "tokenizer_tinybert = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def tinybert_tokenizer_function(dataset):\n",
    "    comments = [str(comment) for comment in dataset['text']]\n",
    "    return tokenizer_tinybert(comments, padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize raw datasets for TinyBERT\n",
    "tokenized_datasets_tinybert = datasets.map(tinybert_tokenizer_function, batched=True)\n",
    "\n",
    "# Remove the original 'text' column\n",
    "tokenized_datasets_tinybert = tokenized_datasets_tinybert.remove_columns(['text'])\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_datasets_tinybert.set_format('torch')\n",
    "\n",
    "# Check the first tokenized example in the train split\n",
    "print(tokenized_datasets_tinybert['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training arguments (adjust output_dir as needed)\n",
    "training_args_tinybert = TrainingArguments(\n",
    "    output_dir='./tinybert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_score',\n",
    "    greater_is_better=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Metrics function (same as ALBERT)\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = numpy.argmax(logits, axis=-1)\n",
    "    Accuracy_score = accuracy_score(labels, predictions)\n",
    "    F1_score = f1_score(labels, predictions, average='weighted')\n",
    "    return {'accuracy_score': Accuracy_score, 'f1_score': F1_score}\n",
    "\n",
    "# Assuming you have TinyBERT-tokenized datasets\n",
    "trainer_tinybert = Trainer(\n",
    "    model=model_tinybert,\n",
    "    args=training_args_tinybert,\n",
    "    train_dataset=tokenized_datasets_tinybert['train'],\n",
    "    eval_dataset=tokenized_datasets_tinybert['test'],\n",
    "    tokenizer=tokenizer_tinybert,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_tinybert.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.783484Z",
     "iopub.status.idle": "2025-05-07T01:49:54.783753Z",
     "shell.execute_reply": "2025-05-07T01:49:54.783638Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.783622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "distilbert_metrics = trainer_distilbert.evaluate()\n",
    "print(\"DistilBERT eval metrics:\", distilbert_metrics)\n",
    "electra_metrics = trainer_electra.evaluate()\n",
    "print(\"ELECTRA eval metrics:\", electra_metrics)\n",
    "albert_metrics = trainer_albert.evaluate()\n",
    "print(\"ALBERT eval metrics:\", albert_metrics)\n",
    "tinybert_metrics = trainer_tinybert.evaluate()\n",
    "print(\"TinyBERT eval metrics:\", tinybert_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.784595Z",
     "iopub.status.idle": "2025-05-07T01:49:54.784897Z",
     "shell.execute_reply": "2025-05-07T01:49:54.784732Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.784717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Model\": [\"DistilBERT\", \"ELECTRA\", \"ALBERT\", \"TinyBERT\"],\n",
    "    \"Accuracy\": [\n",
    "        distilbert_metrics.get(\"eval_accuracy_score\"),\n",
    "        electra_metrics.get(\"eval_accuracy_score\"),\n",
    "        albert_metrics.get(\"eval_accuracy_score\"),\n",
    "        tinybert_metrics.get(\"eval_accuracy_score\"),\n",
    "    ],\n",
    "    \"F1 Score\": [\n",
    "        distilbert_metrics.get(\"eval_f1_score\"),\n",
    "        electra_metrics.get(\"eval_f1_score\"),\n",
    "        albert_metrics.get(\"eval_f1_score\"),\n",
    "        tinybert_metrics.get(\"eval_f1_score\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(data)\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.785498Z",
     "iopub.status.idle": "2025-05-07T01:49:54.785694Z",
     "shell.execute_reply": "2025-05-07T01:49:54.785609Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.785600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Added one more color for TinyBERT\n",
    "colors = [\"skyblue\", \"orange\", \"green\", \"purple\"]  # customize as you like\n",
    "\n",
    "axes[0].bar(df_metrics[\"Model\"], df_metrics[\"Accuracy\"], color=colors)\n",
    "axes[0].set_title(\"Model Accuracy\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axes[1].bar(df_metrics[\"Model\"], df_metrics[\"F1 Score\"], color=colors)\n",
    "axes[1].set_title(\"Model F1 Score (Weighted)\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_ylabel(\"F1 Score\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model based on F1-score\n",
    "Comparing models based on F1-score is ideal because it balances precision and recall, providing a more comprehensive measure of performance—especially for imbalanced datasets—than accuracy alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.786673Z",
     "iopub.status.idle": "2025-05-07T01:49:54.786971Z",
     "shell.execute_reply": "2025-05-07T01:49:54.786833Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.786818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Find the best model name based on F1 Score\n",
    "best_model_name = df_metrics.loc[df_metrics['F1 Score'].idxmax(), 'Model']\n",
    "\n",
    "# Map model name to your actual model object\n",
    "model_map = {\n",
    "    'DistilBERT': trainer_distilbert.model,\n",
    "    'ELECTRA': trainer_electra.model,\n",
    "    'ALBERT': trainer_albert.model,\n",
    "    'TinyBERT': trainer_tinybert.model  \n",
    "}\n",
    "\n",
    "# Save the best model to variable 'model'\n",
    "model = model_map[best_model_name]\n",
    "\n",
    "# Get row with best model metrics\n",
    "best_metrics = df_metrics[df_metrics['Model'] == best_model_name].iloc[0]\n",
    "\n",
    "print(f\"Conclusion: The best model is **{best_model_name}** \"\n",
    "      f\"with an F1 Score of {best_metrics['F1 Score']:.4f} \"\n",
    "      f\"and an Accuracy of {best_metrics['Accuracy']:.4f}.\")\n",
    "\n",
    "print(f\"Saved the best model ({best_model_name}) to variable 'model'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.788124Z",
     "iopub.status.idle": "2025-05-07T01:49:54.788443Z",
     "shell.execute_reply": "2025-05-07T01:49:54.788301Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.788287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('./bestModel/youtube-sentiment-model')\n",
    "tokenizer.save_pretrained('./bestModel/youtube-sentiment-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.788977Z",
     "iopub.status.idle": "2025-05-07T01:49:54.789306Z",
     "shell.execute_reply": "2025-05-07T01:49:54.789164Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.789150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_classifier = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='./bestModel/youtube-sentiment-model',\n",
    "    tokenizer='./bestModel/youtube-sentiment-model',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "label_map = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.790082Z",
     "iopub.status.idle": "2025-05-07T01:49:54.790368Z",
     "shell.execute_reply": "2025-05-07T01:49:54.790269Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.790256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_comments = [\n",
    "    \"This tutorial is fantastic and extremely helpful!\",\n",
    "    \"I'm a bit confused by some parts of this explanation.\",\n",
    "    \"The content is decent, not particularly good or bad.\"\n",
    "]\n",
    "\n",
    "predictions = sentiment_classifier(new_comments)\n",
    "\n",
    "for comment, prediction in zip(new_comments, predictions):\n",
    "    predicted_label_str = prediction['label'] \n",
    "    predicted_label_int = int(predicted_label_str.split('_')[-1])\n",
    "    sentiment = label_map[predicted_label_int]\n",
    "    confidence = prediction['score']\n",
    "    \n",
    "    print(f'Comment: \"{comment}\"')\n",
    "    print(f'Predicted Sentiment: {sentiment} (Confidence: {confidence:.4f})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment():\n",
    "    print(\"Type a comment to analyze its sentiment (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        user_input = input(\"Your comment: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting sentiment prediction.\")\n",
    "            break\n",
    "\n",
    "        prediction = sentiment_classifier(user_input)[0]\n",
    "        label_id = int(prediction['label'].split('_')[-1])\n",
    "        confidence = prediction['score']\n",
    "\n",
    "        sentiment = {\n",
    "            0: \"Negative\",\n",
    "            1: \"Neutral\",\n",
    "            2: \"Positive\"\n",
    "        }.get(label_id, \"Unknown\")\n",
    "\n",
    "        print(f\"\\nInput: \\\"{user_input}\\\"\")\n",
    "        print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.4f})\\n\")\n",
    "\n",
    "predict_sentiment()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6560918,
     "sourceId": 10599713,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
