{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing ELECTRA, ALBERT, DistilBERT, and TinyBERT for YouTube Comment Sentiment Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 1. DistilBERT: Distillation-Based Compression\n",
    "\n",
    "**How:**  \n",
    "DistilBERT uses *knowledge distillation* — training a smaller student model to mimic a larger BERT teacher.\n",
    "\n",
    "**Why Interesting:**  \n",
    "- Achieves about 40% fewer parameters and 60% faster inference while retaining around 97% of BERT’s performance.  \n",
    "- Illustrates simple but effective model compression via distillation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ALBERT: Parameter Sharing & Factorization\n",
    "\n",
    "**How:**  \n",
    "ALBERT reduces model size by *sharing parameters across layers* and *factorizing embeddings*.\n",
    "\n",
    "**Why Interesting:**  \n",
    "- Dramatically reduces the number of parameters without heavily impacting capacity.  \n",
    "- Introduces *sentence-order prediction* to improve pretraining efficiency.  \n",
    "- Shows that *architectural changes* (not just distillation) can yield compact, fast, yet strong models.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ELECTRA: Efficient Pretraining via Discriminators\n",
    "\n",
    "**How:**  \n",
    "ELECTRA replaces masked token prediction with a *replaced token detection* task, where a generator replaces some tokens and a discriminator predicts which tokens were replaced.\n",
    "\n",
    "**Why Interesting:**  \n",
    "- Makes pretraining more sample-efficient and faster to converge.  \n",
    "- Smaller ELECTRA models often outperform comparable-sized BERTs despite using fewer compute resources during training.  \n",
    "- Highlights innovation in *pretraining objectives* rather than model size or architecture alone.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. TinyBERT: Distillation with Layer-Wise Compression\n",
    "\n",
    "**How:**  \n",
    "TinyBERT applies *knowledge distillation* focusing on both *transformer layer compression* and *embedding compression*, using a two-stage distillation from both the encoder and prediction layers.\n",
    "\n",
    "**Why Interesting:**  \n",
    "- Produces a very compact model with fewer layers (typically 4 or 6), substantially reducing size and latency.  \n",
    "- Maintains strong performance close to larger BERT models on various NLP tasks including sentiment analysis.  \n",
    "- Designed specifically for deployment on resource-constrained devices, balancing speed and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Compare These?\n",
    "\n",
    "- They represent **complementary approaches** to making transformers faster and lighter:  \n",
    "  - Distillation (DistilBERT, TinyBERT)  \n",
    "  - Parameter efficiency (ALBERT)  \n",
    "  - Pretraining objective redesign (ELECTRA)  \n",
    "- Comparing accuracy, speed, size, and resource consumption on the same tasks reveals trade-offs important for real applications — especially on resource-restricted devices.  \n",
    "- Helps practitioners **choose the best fit** for their particular constraints (e.g., mobile deployment vs. cloud inference).  \n",
    "- Informs **future model design** by highlighting which efficiency techniques work best in which contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:17.638165Z",
     "iopub.status.busy": "2025-05-07T01:29:17.637902Z",
     "iopub.status.idle": "2025-05-07T01:29:45.195872Z",
     "shell.execute_reply": "2025-05-07T01:29:45.195309Z",
     "shell.execute_reply.started": "2025-05-07T01:29:17.638119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\College\\Semester 4\\Natural Language Processing\\Assignment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification as SeqModClf\n",
    "import torch\n",
    "import numpy \n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.197444Z",
     "iopub.status.busy": "2025-05-07T01:29:45.196913Z",
     "iopub.status.idle": "2025-05-07T01:29:45.201233Z",
     "shell.execute_reply": "2025-05-07T01:29:45.200528Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.197425Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.202020Z",
     "iopub.status.busy": "2025-05-07T01:29:45.201802Z",
     "iopub.status.idle": "2025-05-07T01:29:45.339219Z",
     "shell.execute_reply": "2025-05-07T01:29:45.338575Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.201996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets not forget that apple pay in 2014 require...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50 of retailers don’t even have con...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Sentiment\n",
       "0  lets not forget that apple pay in 2014 require...   neutral\n",
       "1  here in nz 50 of retailers don’t even have con...  negative\n",
       "2  i will forever acknowledge this channel with t...  positive\n",
       "3  whenever i go to a place that doesn’t take app...  negative\n",
       "4  apple pay is so convenient secure and easy to ...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"YoutubeCommentsDataSet.csv\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.341230Z",
     "iopub.status.busy": "2025-05-07T01:29:45.340647Z",
     "iopub.status.idle": "2025-05-07T01:29:45.362026Z",
     "shell.execute_reply": "2025-05-07T01:29:45.361367Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.341210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18408 entries, 0 to 18407\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Comment    18364 non-null  object\n",
      " 1   Sentiment  18408 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 287.8+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 18,408 rows and 2 columns: \"Comment\" (with 18,364 non-null text entries) and \"Sentiment\" (fully populated with sentiment labels). Both columns contain text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.362895Z",
     "iopub.status.busy": "2025-05-07T01:29:45.362710Z",
     "iopub.status.idle": "2025-05-07T01:29:45.375316Z",
     "shell.execute_reply": "2025-05-07T01:29:45.374654Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.362880Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comment      44\n",
       "Sentiment     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.376345Z",
     "iopub.status.busy": "2025-05-07T01:29:45.376071Z",
     "iopub.status.idle": "2025-05-07T01:29:45.394199Z",
     "shell.execute_reply": "2025-05-07T01:29:45.393596Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.376324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets not forget that apple pay in 2014 require...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50 of retailers don’t even have con...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18403</th>\n",
       "      <td>i really like the point about engineering tool...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18404</th>\n",
       "      <td>i’ve just started exploring this field and thi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18405</th>\n",
       "      <td>excelente video con una pregunta filosófica pr...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18406</th>\n",
       "      <td>hey daniel just discovered your channel a coup...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18407</th>\n",
       "      <td>this is great focus is key a playful approach ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18364 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Comment Sentiment\n",
       "0      lets not forget that apple pay in 2014 require...   neutral\n",
       "1      here in nz 50 of retailers don’t even have con...  negative\n",
       "2      i will forever acknowledge this channel with t...  positive\n",
       "3      whenever i go to a place that doesn’t take app...  negative\n",
       "4      apple pay is so convenient secure and easy to ...  positive\n",
       "...                                                  ...       ...\n",
       "18403  i really like the point about engineering tool...  positive\n",
       "18404  i’ve just started exploring this field and thi...  positive\n",
       "18405  excelente video con una pregunta filosófica pr...   neutral\n",
       "18406  hey daniel just discovered your channel a coup...  positive\n",
       "18407  this is great focus is key a playful approach ...  positive\n",
       "\n",
       "[18364 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep only the comments in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(str(text)) == \"en\"\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "dataset['is_english'] = dataset['Comment'].apply(is_english)\n",
    "dataset = dataset[dataset['is_english']]\n",
    "dataset = dataset[['Comment', 'Sentiment']]\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downscale the positive value and upscale the negative value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "positive    10642\n",
       "neutral      3319\n",
       "negative     2296\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "neutral     3319\n",
      "positive    3319\n",
      "negative    3319\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_positive = dataset[dataset['Sentiment'] == 'positive']\n",
    "df_neutral = dataset[dataset['Sentiment'] == 'neutral']\n",
    "df_negative = dataset[dataset['Sentiment'] == 'negative']\n",
    "\n",
    "\n",
    "df_positive_downsampled = resample(df_positive,\n",
    "                                   replace=False,\n",
    "                                   n_samples=3319,\n",
    "                                   random_state=42)\n",
    "\n",
    "df_negative_upsampled = resample(df_negative,\n",
    "                                 replace=True,     \n",
    "                                 n_samples=3319,\n",
    "                                 random_state=42)\n",
    "\n",
    "dataset = pd.concat([df_positive_downsampled, df_neutral, df_negative_upsampled])\n",
    "dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(dataset['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "Negative = 0\n",
    "Neutral = 1 \n",
    "Positive = 2\n",
    "\n",
    "To ensure consistent and clear mapping of sentiment categories to numbers across all models, which helps fairly compare their performance since they all work with the same standardized numeric labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.395007Z",
     "iopub.status.busy": "2025-05-07T01:29:45.394778Z",
     "iopub.status.idle": "2025-05-07T01:29:45.414440Z",
     "shell.execute_reply": "2025-05-07T01:29:45.413875Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.394987Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imanu\\AppData\\Local\\Temp\\ipykernel_2272\\3674117635.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Sentiment'] = dataset['Sentiment'].replace({'negative':0,'neutral':1,'positive':2})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seriously</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when some random people are able to teach this...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>841 diagram venn 1508 ds pathway 1940 role of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we want the blood in the streets this is actua...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i wish ethan would meet andrew tate in real li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Sentiment\n",
       "0                                          seriously          1\n",
       "1  when some random people are able to teach this...          1\n",
       "2  841 diagram venn 1508 ds pathway 1940 role of ...          1\n",
       "3  we want the blood in the streets this is actua...          2\n",
       "4  i wish ethan would meet andrew tate in real li...          1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode train & test\n",
    "def encode_labels(dataset):\n",
    "     dataset['Sentiment'] = dataset['Sentiment'].replace({'negative':0,'neutral':1,'positive':2})\n",
    "     return dataset\n",
    "\n",
    "encoded_dataset = encode_labels(dataset)\n",
    "\n",
    "encoded_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.415227Z",
     "iopub.status.busy": "2025-05-07T01:29:45.415014Z",
     "iopub.status.idle": "2025-05-07T01:29:45.419395Z",
     "shell.execute_reply": "2025-05-07T01:29:45.418629Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.415211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_dataset = encoded_dataset.rename(columns={'Comment': 'text', 'Sentiment': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.420278Z",
     "iopub.status.busy": "2025-05-07T01:29:45.420036Z",
     "iopub.status.idle": "2025-05-07T01:29:45.433861Z",
     "shell.execute_reply": "2025-05-07T01:29:45.433183Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.420258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seriously</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when some random people are able to teach this...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>841 diagram venn 1508 ds pathway 1940 role of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we want the blood in the streets this is actua...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i wish ethan would meet andrew tate in real li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                                          seriously      1\n",
       "1  when some random people are able to teach this...      1\n",
       "2  841 diagram venn 1508 ds pathway 1940 role of ...      1\n",
       "3  we want the blood in the streets this is actua...      2\n",
       "4  i wish ethan would meet andrew tate in real li...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "Allocate 30% of  data for testing and 70% for training, which provides a balanced split that allows enough data for the model to learn while reserving a sizable portion to reliably evaluate its performance on unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.436412Z",
     "iopub.status.busy": "2025-05-07T01:29:45.436221Z",
     "iopub.status.idle": "2025-05-07T01:29:45.567180Z",
     "shell.execute_reply": "2025-05-07T01:29:45.566656Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.436398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(encoded_dataset, test_size = 0.3, \n",
    "                               random_state = 42, \n",
    "                               stratify = encoded_dataset['label'])\n",
    "\n",
    "train.to_csv(\"train.csv\", index=True)\n",
    "test.to_csv(\"test.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T00:45:07.943337Z",
     "iopub.status.busy": "2025-05-07T00:45:07.943033Z",
     "iopub.status.idle": "2025-05-07T00:45:07.947336Z",
     "shell.execute_reply": "2025-05-07T00:45:07.946383Z",
     "shell.execute_reply.started": "2025-05-07T00:45:07.943318Z"
    }
   },
   "source": [
    "## Conver pandas dataset to HuggingFace dataset\n",
    "Hugging Face offers specialized, integrated support for transformer models, including standardized evaluation metrics, easy access to pretrained models, and streamlined training/evaluation pipelines—making model comparison more efficient, consistent, and tailored for NLP tasks than general-purpose pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.567975Z",
     "iopub.status.busy": "2025-05-07T01:29:45.567807Z",
     "iopub.status.idle": "2025-05-07T01:29:45.616261Z",
     "shell.execute_reply": "2025-05-07T01:29:45.615556Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.567961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_hf = Dataset.from_pandas(train.reset_index(drop= True))\n",
    "test_hf = Dataset.from_pandas(test.reset_index(drop= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.617542Z",
     "iopub.status.busy": "2025-05-07T01:29:45.617310Z",
     "iopub.status.idle": "2025-05-07T01:29:45.625412Z",
     "shell.execute_reply": "2025-05-07T01:29:45.624700Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.617519Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['anna karenina and the death of ivan ilyich are among the best books ive ever read if you do read tolstoy make sure to go with the pevear and volokhonsky translations they allow you to get the most from his works'], 'label': [2]}\n"
     ]
    }
   ],
   "source": [
    "print(train_hf[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.626372Z",
     "iopub.status.busy": "2025-05-07T01:29:45.626182Z",
     "iopub.status.idle": "2025-05-07T01:29:45.633410Z",
     "shell.execute_reply": "2025-05-07T01:29:45.632669Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.626358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['im gonna add myself to the many thank yous im one hour in and i already understood more than i did in a two days course i got from work you singlehandedly gave me the trust that even i can learn python and programming this video is worth gold to me right now thank you mosh'], 'label': [2]}\n"
     ]
    }
   ],
   "source": [
    "print(test_hf[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:45.634534Z",
     "iopub.status.busy": "2025-05-07T01:29:45.634186Z",
     "iopub.status.idle": "2025-05-07T01:29:45.645350Z",
     "shell.execute_reply": "2025-05-07T01:29:45.644594Z",
     "shell.execute_reply.started": "2025-05-07T01:29:45.634513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets = DatasetDict({ 'train': train_hf, 'test' : test_hf})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T03:13:41.422212Z",
     "iopub.status.busy": "2025-05-07T03:13:41.421842Z",
     "iopub.status.idle": "2025-05-07T03:13:50.325626Z",
     "shell.execute_reply": "2025-05-07T03:13:50.324660Z",
     "shell.execute_reply.started": "2025-05-07T03:13:41.422161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer_distilbert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "model = SeqModClf.from_pretrained('distilbert-base-uncased', num_labels = 3)\n",
    "\n",
    "device  =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f'model moved to {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6969/6969 [00:01<00:00, 5200.75 examples/s]\n",
      "Map: 100%|██████████| 2988/2988 [00:00<00:00, 5354.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'anna karenina and the death of ivan ilyich are among the best books ive ever read if you do read tolstoy make sure to go with the pevear and volokhonsky translations they allow you to get the most from his works', 'label': 2, 'input_ids': [101, 4698, 8129, 3981, 1998, 1996, 2331, 1997, 7332, 6335, 10139, 2818, 2024, 2426, 1996, 2190, 2808, 4921, 2063, 2412, 3191, 2065, 2017, 2079, 3191, 2000, 4877, 29578, 2191, 2469, 2000, 2175, 2007, 1996, 21877, 3726, 2906, 1998, 5285, 6559, 8747, 5874, 11913, 2027, 3499, 2017, 2000, 2131, 1996, 2087, 2013, 2010, 2573, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenizer_function(dataset):\n",
    "    comments = [str(comment) for comment in dataset['text']]\n",
    "    return tokenizer(comments, padding = 'max_length',truncation = True)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenizer_function, batched= True)\n",
    "\n",
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:29:53.044840Z",
     "iopub.status.busy": "2025-05-07T01:29:53.044450Z",
     "iopub.status.idle": "2025-05-07T01:49:51.750000Z",
     "shell.execute_reply": "2025-05-07T01:49:51.749387Z",
     "shell.execute_reply.started": "2025-05-07T01:29:53.044813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imanu\\AppData\\Local\\Temp\\ipykernel_2272\\1102259049.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_distilbert = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2616' max='2616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2616/2616 09:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.513772</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.802019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.558434</td>\n",
       "      <td>0.836680</td>\n",
       "      <td>0.835166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.645923</td>\n",
       "      <td>0.854083</td>\n",
       "      <td>0.854971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2616, training_loss=0.38574642216393706, metrics={'train_runtime': 591.5847, 'train_samples_per_second': 35.341, 'train_steps_per_second': 4.422, 'total_flos': 2769545293728768.0, 'train_loss': 0.38574642216393706, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './distilbert',\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    eval_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'eval_f1_score',\n",
    "    greater_is_better = True,\n",
    "    report_to = 'none',\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = numpy.argmax(logits, axis = -1)\n",
    "    Accuracy_score = accuracy_score(labels,predictions)\n",
    "    F1_score = f1_score(labels, predictions, average = 'weighted')\n",
    "    return {'accuracy_score':Accuracy_score,'f1_score':F1_score}\n",
    "\n",
    "trainer_distilbert = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['test'],\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer_distilbert.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELECTRA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:49:51.750918Z",
     "iopub.status.busy": "2025-05-07T01:49:51.750673Z",
     "iopub.status.idle": "2025-05-07T01:49:54.564159Z",
     "shell.execute_reply": "2025-05-07T01:49:54.563547Z",
     "shell.execute_reply.started": "2025-05-07T01:49:51.750898Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "tokenizer_electra = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "model_electra = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=3)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_electra.to(device)\n",
    "print(f'Model moved to {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6969/6969 [00:05<00:00, 1164.02 examples/s]\n",
      "Map: 100%|██████████| 2988/2988 [00:02<00:00, 1133.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(2), 'input_ids': tensor([  101,  4698,  8129,  3981,  1998,  1996,  2331,  1997,  7332,  6335,\n",
      "        10139,  2818,  2024,  2426,  1996,  2190,  2808,  4921,  2063,  2412,\n",
      "         3191,  2065,  2017,  2079,  3191,  2000,  4877, 29578,  2191,  2469,\n",
      "         2000,  2175,  2007,  1996, 21877,  3726,  2906,  1998,  5285,  6559,\n",
      "         8747,  5874, 11913,  2027,  3499,  2017,  2000,  2131,  1996,  2087,\n",
      "         2013,  2010,  2573,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def electra_tokenizer_function(dataset):\n",
    "    comments = [str(comment) for comment in dataset['text']]\n",
    "    return tokenizer_electra(comments, padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenize raw datasets for ELECTRA\n",
    "tokenized_datasets_electra = datasets.map(electra_tokenizer_function, batched=True)\n",
    "\n",
    "# Remove the original 'text' column\n",
    "tokenized_datasets_electra = tokenized_datasets_electra.remove_columns(['text'])\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_datasets_electra.set_format('torch')\n",
    "\n",
    "# Check the first tokenized example in the train split\n",
    "print(tokenized_datasets_electra['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imanu\\AppData\\Local\\Temp\\ipykernel_2272\\2713297849.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_electra = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2616' max='2616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2616/2616 11:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.776500</td>\n",
       "      <td>0.494529</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.811526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.376300</td>\n",
       "      <td>0.521667</td>\n",
       "      <td>0.858434</td>\n",
       "      <td>0.857105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.191400</td>\n",
       "      <td>0.537330</td>\n",
       "      <td>0.883199</td>\n",
       "      <td>0.883492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2616, training_loss=0.410942684984353, metrics={'train_runtime': 709.7585, 'train_samples_per_second': 29.456, 'train_steps_per_second': 3.686, 'total_flos': 5500912224439296.0, 'train_loss': 0.410942684984353, 'epoch': 3.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./electra',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_score',\n",
    "    greater_is_better=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = numpy.argmax(logits, axis=-1)\n",
    "    Accuracy_score = accuracy_score(labels, predictions)\n",
    "    F1_score = f1_score(labels, predictions, average='weighted')\n",
    "    return {'accuracy_score': Accuracy_score, 'f1_score': F1_score}\n",
    "\n",
    "trainer_electra = Trainer(\n",
    "    model=model_electra,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_electra['train'],\n",
    "    eval_dataset=tokenized_datasets_electra['test'],\n",
    "    tokenizer=tokenizer_electra,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_electra.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.781075Z",
     "iopub.status.idle": "2025-05-07T01:49:54.781318Z",
     "shell.execute_reply": "2025-05-07T01:49:54.781220Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.781210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "\n",
    "tokenizer_albert = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "model_albert = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=3)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_albert.to(device)\n",
    "print(f'Model moved to {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6969/6969 [00:02<00:00, 3137.77 examples/s]\n",
      "Map: 100%|██████████| 2988/2988 [00:00<00:00, 3233.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(2), 'input_ids': tensor([    2,  2823,  7512,  1673,    17,    14,   372,    16,  5489,    31,\n",
      "          102,  3870,    50,   497,    14,   246,   964,  5568,   462,  1302,\n",
      "          100,    42,   107,  1302,    20, 24239,  7452,   233,   562,    20,\n",
      "          162,    29,    14,  3560,   195,   512,    17,  2250,   111, 23925,\n",
      "         2397, 13610,    59,  1655,    42,    20,   164,    14,   127,    37,\n",
      "           33,   693,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load ALBERT tokenizer\n",
    "tokenizer_albert = AutoTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "def albert_tokenizer_function(dataset):\n",
    "    comments = [str(comment) for comment in dataset['text']]\n",
    "    return tokenizer_albert(comments, padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenize raw datasets for ALBERT\n",
    "tokenized_datasets_albert = datasets.map(albert_tokenizer_function, batched=True)\n",
    "\n",
    "# Remove the original 'text' column\n",
    "tokenized_datasets_albert = tokenized_datasets_albert.remove_columns(['text'])\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_datasets_albert.set_format('torch')\n",
    "\n",
    "# Check the first tokenized example in the train split\n",
    "print(tokenized_datasets_albert['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.782291Z",
     "iopub.status.idle": "2025-05-07T01:49:54.782605Z",
     "shell.execute_reply": "2025-05-07T01:49:54.782456Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.782443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imanu\\AppData\\Local\\Temp\\ipykernel_2272\\4073434684.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_albert = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2616' max='2616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2616/2616 18:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>0.736903</td>\n",
       "      <td>0.708835</td>\n",
       "      <td>0.697853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>0.620559</td>\n",
       "      <td>0.762383</td>\n",
       "      <td>0.762810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.607423</td>\n",
       "      <td>0.795850</td>\n",
       "      <td>0.798116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2616, training_loss=0.6679870054262493, metrics={'train_runtime': 1119.0662, 'train_samples_per_second': 18.683, 'train_steps_per_second': 2.338, 'total_flos': 499687003524096.0, 'train_loss': 0.6679870054262493, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./albert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_score',\n",
    "    greater_is_better=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = numpy.argmax(logits, axis=-1)\n",
    "    Accuracy_score = accuracy_score(labels, predictions)\n",
    "    F1_score = f1_score(labels, predictions, average='weighted')\n",
    "    return {'accuracy_score': Accuracy_score, 'f1_score': F1_score}\n",
    "\n",
    "trainer_albert = Trainer(\n",
    "    model=model_albert,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_albert['train'],\n",
    "    eval_dataset=tokenized_datasets_albert['test'],\n",
    "    tokenizer=tokenizer_albert,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_albert.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "okenizer_tinybert = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "model_tinybert = AutoModelForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=3)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_tinybert.to(device)\n",
    "print(f'Model moved to {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6969/6969 [00:01<00:00, 3571.97 examples/s]\n",
      "Map: 100%|██████████| 2988/2988 [00:00<00:00, 3399.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(2), 'input_ids': tensor([  101,  4698,  8129,  3981,  1998,  1996,  2331,  1997,  7332,  6335,\n",
      "        10139,  2818,  2024,  2426,  1996,  2190,  2808,  4921,  2063,  2412,\n",
      "         3191,  2065,  2017,  2079,  3191,  2000,  4877, 29578,  2191,  2469,\n",
      "         2000,  2175,  2007,  1996, 21877,  3726,  2906,  1998,  5285,  6559,\n",
      "         8747,  5874, 11913,  2027,  3499,  2017,  2000,  2131,  1996,  2087,\n",
      "         2013,  2010,  2573,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load TinyBERT tokenizer - use the appropriate TinyBERT checkpoint\n",
    "tokenizer_tinybert = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def tinybert_tokenizer_function(dataset):\n",
    "    comments = [str(comment) for comment in dataset['text']]\n",
    "    return tokenizer_tinybert(comments, padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize raw datasets for TinyBERT\n",
    "tokenized_datasets_tinybert = datasets.map(tinybert_tokenizer_function, batched=True)\n",
    "\n",
    "# Remove the original 'text' column\n",
    "tokenized_datasets_tinybert = tokenized_datasets_tinybert.remove_columns(['text'])\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_datasets_tinybert.set_format('torch')\n",
    "\n",
    "# Check the first tokenized example in the train split\n",
    "print(tokenized_datasets_tinybert['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imanu\\AppData\\Local\\Temp\\ipykernel_2272\\885667533.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_tinybert = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2616' max='2616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2616/2616 03:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.581269</td>\n",
       "      <td>0.765395</td>\n",
       "      <td>0.761287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.500400</td>\n",
       "      <td>0.588607</td>\n",
       "      <td>0.794511</td>\n",
       "      <td>0.788036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.601350</td>\n",
       "      <td>0.814926</td>\n",
       "      <td>0.813804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2616, training_loss=0.5290130839799887, metrics={'train_runtime': 199.845, 'train_samples_per_second': 104.616, 'train_steps_per_second': 13.09, 'total_flos': 299805496888320.0, 'train_loss': 0.5290130839799887, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments (adjust output_dir as needed)\n",
    "training_args_tinybert = TrainingArguments(\n",
    "    output_dir='./tinybert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_score',\n",
    "    greater_is_better=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Metrics function (same as ALBERT)\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = numpy.argmax(logits, axis=-1)\n",
    "    Accuracy_score = accuracy_score(labels, predictions)\n",
    "    F1_score = f1_score(labels, predictions, average='weighted')\n",
    "    return {'accuracy_score': Accuracy_score, 'f1_score': F1_score}\n",
    "\n",
    "# Assuming you have TinyBERT-tokenized datasets\n",
    "trainer_tinybert = Trainer(\n",
    "    model=model_tinybert,\n",
    "    args=training_args_tinybert,\n",
    "    train_dataset=tokenized_datasets_tinybert['train'],\n",
    "    eval_dataset=tokenized_datasets_tinybert['test'],\n",
    "    tokenizer=tokenizer_tinybert,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_tinybert.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT eval metrics: {'eval_loss': 0.6459225416183472, 'eval_accuracy_score': 0.8540829986613119, 'eval_f1_score': 0.8549713026166909, 'eval_runtime': 27.208, 'eval_samples_per_second': 109.82, 'eval_steps_per_second': 6.873, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "distilbert_metrics = trainer_distilbert.evaluate()\n",
    "print(\"DistilBERT eval metrics:\", distilbert_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRA eval metrics: {'eval_loss': 0.5373303294181824, 'eval_accuracy_score': 0.8831994645247657, 'eval_f1_score': 0.8834919066058513, 'eval_runtime': 24.3179, 'eval_samples_per_second': 122.873, 'eval_steps_per_second': 7.69, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "electra_metrics = trainer_electra.evaluate()\n",
    "print(\"ELECTRA eval metrics:\", electra_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBERT eval metrics: {'eval_loss': 0.6074225902557373, 'eval_accuracy_score': 0.7958500669344043, 'eval_f1_score': 0.7981163720762566, 'eval_runtime': 48.054, 'eval_samples_per_second': 62.18, 'eval_steps_per_second': 3.891, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "albert_metrics = trainer_albert.evaluate()\n",
    "print(\"ALBERT eval metrics:\", albert_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyBERT eval metrics: {'eval_loss': 0.6013495326042175, 'eval_accuracy_score': 0.8149263721552878, 'eval_f1_score': 0.8138042383855054, 'eval_runtime': 9.2751, 'eval_samples_per_second': 322.154, 'eval_steps_per_second': 20.162, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "tinybert_metrics = trainer_tinybert.evaluate()\n",
    "print(\"TinyBERT eval metrics:\", tinybert_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.784595Z",
     "iopub.status.idle": "2025-05-07T01:49:54.784897Z",
     "shell.execute_reply": "2025-05-07T01:49:54.784732Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.784717Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model  Accuracy  F1 Score\n",
      "0  DistilBERT  0.854083  0.854971\n",
      "1     ELECTRA  0.883199  0.883492\n",
      "2      ALBERT  0.795850  0.798116\n",
      "3    TinyBERT  0.814926  0.813804\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"Model\": [\"DistilBERT\", \"ELECTRA\", \"ALBERT\", \"TinyBERT\"],\n",
    "    \"Accuracy\": [\n",
    "        distilbert_metrics.get(\"eval_accuracy_score\"),\n",
    "        electra_metrics.get(\"eval_accuracy_score\"),\n",
    "        albert_metrics.get(\"eval_accuracy_score\"),\n",
    "        tinybert_metrics.get(\"eval_accuracy_score\"),\n",
    "    ],\n",
    "    \"F1 Score\": [\n",
    "        distilbert_metrics.get(\"eval_f1_score\"),\n",
    "        electra_metrics.get(\"eval_f1_score\"),\n",
    "        albert_metrics.get(\"eval_f1_score\"),\n",
    "        tinybert_metrics.get(\"eval_f1_score\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(data)\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.785498Z",
     "iopub.status.idle": "2025-05-07T01:49:54.785694Z",
     "shell.execute_reply": "2025-05-07T01:49:54.785609Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.785600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Added one more color for TinyBERT\n",
    "colors = [\"skyblue\", \"orange\", \"green\", \"purple\"]  # customize as you like\n",
    "\n",
    "axes[0].bar(df_metrics[\"Model\"], df_metrics[\"Accuracy\"], color=colors)\n",
    "axes[0].set_title(\"Model Accuracy\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axes[1].bar(df_metrics[\"Model\"], df_metrics[\"F1 Score\"], color=colors)\n",
    "axes[1].set_title(\"Model F1 Score (Weighted)\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_ylabel(\"F1 Score\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model based on F1-score\n",
    "Comparing models based on F1-score is ideal because it balances precision and recall, providing a more comprehensive measure of performance—especially for imbalanced datasets—than accuracy alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.786673Z",
     "iopub.status.idle": "2025-05-07T01:49:54.786971Z",
     "shell.execute_reply": "2025-05-07T01:49:54.786833Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.786818Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conclusion: The best model is **ELECTRA** with an F1 Score of 0.8835 and an Accuracy of 0.8832.\n",
      "Saved the best model (ELECTRA) to variable 'model'.\n"
     ]
    }
   ],
   "source": [
    "# Find the best model name based on F1 Score\n",
    "best_model_name = df_metrics.loc[df_metrics['F1 Score'].idxmax(), 'Model']\n",
    "\n",
    "# Map model name to your actual model object\n",
    "model_map = {\n",
    "    'DistilBERT': trainer_distilbert.model,\n",
    "    'ELECTRA': trainer_electra.model,\n",
    "    'ALBERT': trainer_albert.model,\n",
    "    'TinyBERT': trainer_tinybert.model  \n",
    "}\n",
    "\n",
    "# Save the best model to variable 'model'\n",
    "model = model_map[best_model_name]\n",
    "\n",
    "# Get row with best model metrics\n",
    "best_metrics = df_metrics[df_metrics['Model'] == best_model_name].iloc[0]\n",
    "\n",
    "print(f\"Conclusion: The best model is **{best_model_name}** \"\n",
    "      f\"with an F1 Score of {best_metrics['F1 Score']:.4f} \"\n",
    "      f\"and an Accuracy of {best_metrics['Accuracy']:.4f}.\")\n",
    "\n",
    "print(f\"Saved the best model ({best_model_name}) to variable 'model'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.788124Z",
     "iopub.status.idle": "2025-05-07T01:49:54.788443Z",
     "shell.execute_reply": "2025-05-07T01:49:54.788301Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.788287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bestModel/youtube-sentiment-model\\\\tokenizer_config.json',\n",
       " './bestModel/youtube-sentiment-model\\\\special_tokens_map.json',\n",
       " './bestModel/youtube-sentiment-model\\\\vocab.txt',\n",
       " './bestModel/youtube-sentiment-model\\\\added_tokens.json',\n",
       " './bestModel/youtube-sentiment-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./bestModel/youtube-sentiment-model')\n",
    "tokenizer.save_pretrained('./bestModel/youtube-sentiment-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.788977Z",
     "iopub.status.idle": "2025-05-07T01:49:54.789306Z",
     "shell.execute_reply": "2025-05-07T01:49:54.789164Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.789150Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_classifier = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='./bestModel/youtube-sentiment-model',\n",
    "    tokenizer='./bestModel/youtube-sentiment-model',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "label_map = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-07T01:49:54.790082Z",
     "iopub.status.idle": "2025-05-07T01:49:54.790368Z",
     "shell.execute_reply": "2025-05-07T01:49:54.790269Z",
     "shell.execute_reply.started": "2025-05-07T01:49:54.790256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: \"This tutorial is fantastic and extremely helpful!\"\n",
      "Predicted Sentiment: positive (Confidence: 0.9989)\n",
      "\n",
      "Comment: \"The meeting is scheduled to start at 10 a.m.\"\n",
      "Predicted Sentiment: neutral (Confidence: 0.9961)\n",
      "\n",
      "Comment: \"The results were not exactly what we had hoped for.\"\n",
      "Predicted Sentiment: negative (Confidence: 0.9980)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_comments = [\n",
    "    \"This tutorial is fantastic and extremely helpful!\",\n",
    "    \"The meeting is scheduled to start at 10 a.m.\",\n",
    "    \"The results were not exactly what we had hoped for.\"\n",
    "]\n",
    "\n",
    "predictions = sentiment_classifier(new_comments)\n",
    "\n",
    "for comment, prediction in zip(new_comments, predictions):\n",
    "    predicted_label_str = prediction['label'] \n",
    "    predicted_label_int = int(predicted_label_str.split('_')[-1])\n",
    "    sentiment = label_map[predicted_label_int]\n",
    "    confidence = prediction['score']\n",
    "    \n",
    "    print(f'Comment: \"{comment}\"')\n",
    "    print(f'Predicted Sentiment: {sentiment} (Confidence: {confidence:.4f})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment():\n",
    "    print(\"Type a comment to analyze its sentiment (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        user_input = input(\"Your comment: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting sentiment prediction.\")\n",
    "            break\n",
    "\n",
    "        prediction = sentiment_classifier(user_input)[0]\n",
    "        label_id = int(prediction['label'].split('_')[-1])\n",
    "        confidence = prediction['score']\n",
    "\n",
    "        sentiment = {\n",
    "            0: \"Negative\",\n",
    "            1: \"Neutral\",\n",
    "            2: \"Positive\"\n",
    "        }.get(label_id, \"Unknown\")\n",
    "\n",
    "        print(f\"\\nInput: \\\"{user_input}\\\"\")\n",
    "        print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.4f})\\n\")\n",
    "\n",
    "predict_sentiment()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6560918,
     "sourceId": 10599713,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
